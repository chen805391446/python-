{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 剩余内容：\n",
    "    - scrapy的五大核心组件介绍\n",
    "    - 请求传参实现的深度爬取\n",
    "    - 中间件机制\n",
    "    - 大文件下载\n",
    "    - CrawlSpider\n",
    "    - 分布式\n",
    "    - 增量式\n",
    "\n",
    "\n",
    "五大核心组件\n",
    "    - 目的：\n",
    "        - 1.大概了解scrapy的运行机制\n",
    "        - 2.分布式铺垫\n",
    "    - 组件的作用：\n",
    "        引擎(Scrapy)\n",
    "            用来处理整个系统的数据流处理, 触发事务(框架核心)\n",
    "        调度器(Scheduler)\n",
    "            用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址\n",
    "        下载器(Downloader)\n",
    "            用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)\n",
    "        爬虫(Spiders)\n",
    "            爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面\n",
    "        项目管道(Pipeline)\n",
    "            负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。\n",
    "\n",
    "## 请求传参实现的深度爬取\n",
    "- 深度爬取：爬取的数据没有在同一张页面中（首页数据+详情页数据）\n",
    "- 在scrapy中如果没有请求传参我们是无法持久化存储数据\n",
    "- 实现方式：\n",
    "    - scrapy.Request(url,callback,meta)\n",
    "        - meta是一个字典，可以将meta传递给callback\n",
    "    - callback取出meta：\n",
    "        - response.meta\n",
    "\n",
    "\n",
    "## 中间件：\n",
    "- 作用：批量拦截请求和响应\n",
    "- 爬虫中间件\n",
    "- 下载中间件（推荐）\n",
    "    - 拦截请求：\n",
    "        - 篡改请求url\n",
    "        - 伪装请求头信息\n",
    "            - UA\n",
    "            - Cookie\n",
    "        - 设置请求代理（重点）\n",
    "    - 拦截响应\n",
    "        - 篡改响应数据\n",
    "\n",
    "    - 代理操作必须使用中间件才可以实现\n",
    "        - process_exception：\n",
    "            - request.meta['proxy'] = 'http://ip:port'\n",
    "\n",
    "\n",
    "\n",
    "## 大文件下载：大文件数据是在管道中请求到的\n",
    "    - 下属管道类是scrapy封装好的我们直接用即可：\n",
    "    - from scrapy.pipelines.images import ImagesPipeline #提供了数据下载功能\n",
    "    - 重写该管道类的三个方法：\n",
    "        - get_media_requests\n",
    "            - 对图片地址发起请求\n",
    "        - file_path\n",
    "            - 返回图片名称即可\n",
    "        - item_completed\n",
    "            - 返回item，将其返回给下一个即将被执行的管道类\n",
    "        - 在配置文件中添加：\n",
    "            - IMAGES_STORE = 'dirName'\n",
    "\n",
    "\n",
    "- settings.py中的常用配置\n",
    "    增加并发：\n",
    "        默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS = 100值为100,并发设置成了为100。\n",
    "\n",
    "    降低日志级别：\n",
    "        在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为INFO或者ERROR即可。在配置文件中编写：LOG_LEVEL = ‘INFO’\n",
    "\n",
    "    禁止cookie：\n",
    "        如果不是真的需要cookie，则在scrapy爬取数据时可以禁止cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED = False\n",
    "\n",
    "    禁止重试：\n",
    "        对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED = False\n",
    "\n",
    "    减少下载超时：\n",
    "        如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT = 10 超时时间为10s\n",
    "\n",
    "\n",
    "- CrawlSpider\n",
    "    - 其实SPider的一个子类。Spider爬虫文件中爬虫类的父类。\n",
    "        - 子类的功能一定是多余父类\n",
    "    - 作用：被用作于专业实现全站数据爬取\n",
    "        - 将一个页面下所有页码对应的数据进行爬取\n",
    "    - 基本使用：\n",
    "        - 1.创建一个工程\n",
    "        - 2.cd 工程\n",
    "        - 3.创建一个基于CrawlSpider的爬虫文件\n",
    "            - scrapy genspider -t crawl SpiderName www.xxx.com\n",
    "        - 4.执行工程\n",
    "    - 注意：\n",
    "        - 1.一个链接提取器对应一个规则解析器（多个链接提取器和多个规则解析器）\n",
    "        - 2.在实现深度爬取的过程中需要和scrapy.Request()结合使用\n",
    "            - 明日讲\n",
    "    - 面试题：\n",
    "        - 如何将一个网站中全站所有的链接都进行爬取。\n",
    "\n",
    "    任务：尝试使用CrawlSpider实现深度爬取。\n",
    "\n",
    "- 分布式\n",
    "- 增量式\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
