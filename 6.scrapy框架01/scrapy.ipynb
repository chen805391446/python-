{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy框架\n",
    "- 简介：所谓的框架其实就是一个被集成了很多功能且具有很强通用性的一个项目模板。\n",
    "- 学习：学习是框架中集成好的各种功能的特性是作用。\n",
    "- 进阶：逐步的探索框架的底层。\n",
    "- scrapy：是一个专门用于异步爬虫的框架。\n",
    "    - 高性能的数据解析、请求发送，持久化存储，全站数据爬取，中间件，分布式......\n",
    "- 环境的安装：\n",
    "    - mac、linum：pip install scrapy\n",
    "    - windows:\n",
    "        - a. pip3 install wheel\n",
    "\n",
    "        - b. 下载twisted文件，下载地址：http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted\n",
    "\n",
    "        - c. 进入下载目录，执行 pip install Twisted‑17.1.0‑cp35‑cp35m‑win_amd64.whl\n",
    "            - Twisted:就是一个异步的架构。被作用在了scrapy中。\n",
    "            - 安装报错：需要更换另一个版本的twisted文件进行安装即可。\n",
    "\n",
    "        - d. pip install pywin32\n",
    "\n",
    "        - e. pip install scrapy\n",
    "    - 测试：cmd中scrapy按下回车，如果没有报错说明安装成功。\n",
    "\n",
    "## scrapy的基本使用：\n",
    "- 创建一个工程：\n",
    "    - scrapy startproject ProName\n",
    "    - 目录结构：\n",
    "        - spiders：爬虫文件夹\n",
    "            - 必须要存放一个爬虫源文件\n",
    "        - settings.py:工程的配置文件\n",
    "- cd ProName\n",
    "- 创建爬虫源文件：\n",
    "    - scrapy genspider spiderName www.xxx.com\n",
    "    - 编写对应的代码在爬虫文件中\n",
    "- 执行工程\n",
    "    - scrapy crawl spiderName\n",
    "    - 执行工程后，默认会输出工程所有的日志信息。\n",
    "    - 指定类型日志的输出：\n",
    "        - settings.py:\n",
    "            - LOG_LEVEL = 'ERROR'\n",
    "\n",
    "## 爬虫文件spiderName内容阐述：\n",
    "    - name：爬虫文件名称，该文件的唯一表示\n",
    "    - start_urls：起始url列表，存储的都是url，url可以被自动进行get请求的发送\n",
    "    - parse方法：请求后的数据解析操作\n",
    "- settings.py:\n",
    "    - 1.禁止robots\n",
    "    - 2.指定日志类型：LOG_LEVEL = 'ERROR'\n",
    "    - 3.UA伪装\n",
    "\n",
    "\n",
    "- scrapy数据解析\n",
    "    - 使用：response.xpath('xpath表达式')\n",
    "    - scrapy封装的xpath和etree中的xpath区别:\n",
    "        - scrapy中的xpath直接将定位到的标签中存储的值或者属性值取出，返回的是Selector对象，且相关\n",
    "          的数据值是存储在Selector对象的data属性中，需要调用extract、extract_first()取出字符串数据\n",
    "\n",
    "- 持久化存储\n",
    "    - 基于终端指令的持久化存储\n",
    "        - 要求：该种方式只可以将parse方法的返回值存储到本地指定后缀的文本文件中。\n",
    "        - 执行指令：scrapy crawl spiderName -o filePath\n",
    "\n",
    "    - 基于管道的持久化存储（重点）\n",
    "        - 在爬虫文件中进行数据解析\n",
    "        - 在items.py中定义相关属性\n",
    "            - 步骤1中解析出了几个字段的数据，在此就定义几个属性\n",
    "        - 在爬虫文件中将解析到的数据存储封装到Item类型的对象中\n",
    "        - 将Item类型的对象提交给管道\n",
    "        - 在管道文件（pipelines.py）中,接收爬虫文件提交过来的Item类型对象，且对其进行任意形式的\n",
    "            持久化存储操作\n",
    "        - 在配置文件中开启管道机制\n",
    "    - 基于管道实现数据的备份\n",
    "        - 将爬取到的数据分别存储到不同的载体。\n",
    "        - 实现：将数据一份存储到mysql，一份存储到redis\n",
    "        - 问题：管道文件中的一个管道类表示怎样的一组操作呢？\n",
    "            - 一个管道类对应一种形式的持久化存储操作。如果将数据存储到不同的载体中就需要使用多个管道类。\n",
    "        - 已经定义好了三个管道类，将数据写入到三个载体中进行存储：\n",
    "            - item会不会依次提交给三个管道类\n",
    "                - 不会，爬虫文件中的item只会被提交给优先级最高的那一个管道类\n",
    "                - 优先级高的管道类需要在process_item中实现return item，就item传递给下一个即将被执行的管道类\n",
    "\n",
    "- scrapy的手动请求发送实现的全站数据爬取\n",
    "    - yield scrapy.Request(url,callback)：GET\n",
    "        - callback指定解析函数，用于解析数据\n",
    "\n",
    "    - yield scrapy.FormRequest(url,callback,formdata):POST\n",
    "        - formdata:字典，请求参数\n",
    "\n",
    "- 为什么start_urls列表中的url会被自动进行get请求的发送？\n",
    "    - 因为列表中的url其实是被start_requests这个父类方法实现的get请求发送\n",
    "    def start_requests(self):\n",
    "        for u in self.start_urls:\n",
    "           yield scrapy.Request(url=u,callback=self.parse)\n",
    "- 如何将start_urls中的url默认进行post请求的发送？\n",
    "    - 重写start_requests方法即可\n",
    "    def start_requests(self):\n",
    "        for u in self.start_urls:\n",
    "           yield scrapy.FormRequest(url=u,callback=self.parse)\n",
    "\n",
    "\n",
    "\n",
    "- 剩余内容：\n",
    "    - scrapy的五大核心组件介绍\n",
    "    - 请求传参实现的深度爬取\n",
    "    - 中间件机制\n",
    "    - CrawlSpider\n",
    "    - 分布式\n",
    "    - 增量式"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
