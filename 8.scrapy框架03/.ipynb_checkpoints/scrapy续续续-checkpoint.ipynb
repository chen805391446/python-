{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "\n",
    "# 今日内容:\n",
    "- CrawlSpider深度爬取\n",
    "- selenium在scrapy中的使用\n",
    "- 分布式\n",
    "- 增量式\n",
    "\n",
    "\n",
    "## CrawlSpider实现的深度爬取\n",
    "    - 通用方式：CrawlSpider+Spider实现\n",
    "\n",
    "## selenium在scrapy中的使用\n",
    "    - https://news.163.com/\n",
    "    - 爬取网易新闻中的国内，国际，军事，航空，无人机这五个板块下所有的新闻数据（标题+内容）\n",
    "- 分析\n",
    "    - 首页没有动态加载的数据\n",
    "        - 爬取五个板块对应的url\n",
    "    - 每一个板块对应的页面中的新闻标题是动态加载\n",
    "        - 爬取新闻标题+详情页的url（***）\n",
    "    - 每一条新闻详情页面中的数据不是动态加载\n",
    "        - 爬取的新闻内容\n",
    "\n",
    "- selenium在scrapy中的使用流程\n",
    "    - 1.在爬虫类中实例化一个浏览器对象，将其作为爬虫类的一个属性\n",
    "    - 2.在中间件中实现浏览器自动化相关的操作\n",
    "    - 3.在爬虫类中重写closed(self,spider),在其内部关闭浏览器对象\n",
    "\n",
    "\n",
    "## 分布式\n",
    "- 实现方式：scrapy+redis（scrapy结合着scrapy-redis组件）\n",
    "- 原生的scrapy框架是无法实现分布式\n",
    "    - 什么是是分布式\n",
    "        - 需要搭建一个分布式的机群，让后让机群中的每一台电脑执行同一组程序，让其对同一组资源\n",
    "            进行联合且分布的数据爬取。\n",
    "    - 为什么原生的scrapy框架无法实现分布式？\n",
    "        - 调度器无法被分布式机群共享\n",
    "        - 管道无法分布式机群被共享\n",
    "    - 如何实现分布式：使用scrapy-redis组件即可\n",
    "    - scrapy-redis组件的作用：\n",
    "        - 可以给原生的scrapy框架提供共享的管道和调度器\n",
    "        - pip install scrapy-redis\n",
    "- 实现流程\n",
    "    1.修改爬虫文件\n",
    "        - 1.1 导包：from scrapy_redis.spiders import RedisCrawlSpider\n",
    "        - 1.2 修改当前爬虫类的父类为：RedisCrawlSpider\n",
    "        - 1.3 将start_url替换成redis_keys的属性，属性值为任意字符串\n",
    "            - redis_key = 'xxx'：表示的是可以被共享的调度器队列的名称，最终是需要将起始的url手动\n",
    "            放置到redis_key表示的队列中\n",
    "        - 1.4 将数据解析的补充完整即可\n",
    "    2.对settings.py进行配置\n",
    "        - 指定调度器\n",
    "            # 增加了一个去重容器类的配置, 作用使用Redis的set集合来存储请求的指纹数据, 从而实现请求去重的持久化\n",
    "            DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\"\n",
    "            # 使用scrapy-redis组件自己的调度器\n",
    "            SCHEDULER = \"scrapy_redis.scheduler.Scheduler\"\n",
    "            # 配置调度器是否要持久化, 也就是当爬虫结束了, 要不要清空Redis中请求队列和去重指纹的set。如果是True, 就表示要持久化存储, 就不清空数据, 否则清空数据\n",
    "            SCHEDULER_PERSIST = True\n",
    "        - 指定管道\n",
    "            ITEM_PIPELINES = {\n",
    "                'scrapy_redis.pipelines.RedisPipeline': 400\n",
    "            }\n",
    "            - 特点：该种管道只可以将item写入redis\n",
    "        - 指定redis\n",
    "            REDIS_HOST = 'redis服务的ip地址'\n",
    "            REDIS_PORT = 6379\n",
    "    3.配置redis的配置文件（redis.window.conf）\n",
    "        - 解除默认绑定\n",
    "            - 56行：#bind 127.0.0.1\n",
    "        - 关闭保护模式\n",
    "            - 75行：protected-mode no\n",
    "     4.启动redis服务和客户端\n",
    "     5.执行scrapy工程（不要在配置文件中加入LOG_LEVEL）\n",
    "        - 程序会停留在listening位置：等待起始的url加入\n",
    "     6.向redis_key表示的队列中添加起始url\n",
    "        - 需要在redis的客户端执行如下指令：（调度器队列是存在于redis中）\n",
    "            - lpush sunQueue http://wz.sun0769.com/political/index/politicsNewest?id=1&page=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 增量式\n",
    "- 概念：监测网站数据更新的情况，以便于爬取到最新更新出来的数据。\n",
    "- 实现核心：去重\n",
    "- 实战中去重的方式：记录表\n",
    "    - 记录表需要记录什么？记录的一定是爬取过的相关信息。\n",
    "        - 爬取过的相关信息：每一部电影详情页的url\n",
    "        - 只需要使用某一组数据，该组数据如果可以作为该部电影的唯一标识即可，刚好电影详情页的url\n",
    "          就可以作为电影的唯一标识。只要可以表示电影唯一标识的数据我们统称为数据指纹。\n",
    "- 去重的方式对应的记录表：\n",
    "    - python中的set集合（不可以）\n",
    "        - set集合无法持久化存储\n",
    "    - redis中的set可以的\n",
    "        - 可以持久化存储\n",
    "\n",
    "- 数据指纹一般是经过加密\n",
    "    - 当前案例的数据指纹没有必要加密。\n",
    "    - 什么情况数据指纹需要加密？\n",
    "        - 如果数据的唯一标识标识的内容数据量比较大，可以使用hash将数据加密成32位的密文。\n",
    "            - 目的是为了节省空间。\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
